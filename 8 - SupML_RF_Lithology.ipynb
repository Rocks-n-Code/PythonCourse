{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "8 - SupML_RF_Lithology.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocks-n-Code/PythonCourse/blob/master/8%20-%20SupML_RF_Lithology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOFBHv4xZrGy"
      },
      "source": [
        "# Supervised Machine Learning: Random Forest Regressor\n",
        "\n",
        "## Building and mineral model from a pulsed neutron log.\n",
        "\n",
        "A random forest model is common method comprised of a group of decision trees. We will be using scikit learn's [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GD-epdiZslW"
      },
      "source": [
        "pip install lasio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNIqRC7JZrGz"
      },
      "source": [
        "import pandas as pd\n",
        "import lasio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import time\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_columns',30)\n",
        "pd.set_option('display.max_rows',10)\n",
        "\n",
        "import warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ECI5nr8ZrG2"
      },
      "source": [
        "For demonstration puropses we are going to use one pused neutron log for training data. If you were to be doing this for a deployable model you'd want to use more training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co7Rj3z6ZrG2"
      },
      "source": [
        "las = lasio.read(\"https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/0504511399_2355551.LAS.txt\")\n",
        "las_df = las.df()\n",
        "\n",
        "#Preview our data\n",
        "las_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWcbs1MzZrG6"
      },
      "source": [
        "#Pull the Non-Walk Curves\n",
        "curves = {}\n",
        "for x in las.curves[1:]:\n",
        "    #Skip Pulsed Neutron Logs\n",
        "    if 'WALK' in x['mnemonic']: continue\n",
        "\n",
        "    curves[x['mnemonic']] = x['descr']\n",
        "\n",
        "print(list(curves.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QfHQ8m0ZrG8"
      },
      "source": [
        "#Our Possible Outcome Columns\n",
        "pn_cols = { \"WANH_WALK2\" : 'Anhydrite',\n",
        "            \"WCAR_WALK2\" : 'Carbonate',\n",
        "            \"WCLA_WALK2\" : 'Clay',\n",
        "            \"WCOA_WALK2\" : 'Coal',\n",
        "            \"WEVA_WALK2\" : 'Salt',\n",
        "            \"WPYR_WALK2\" : 'Pyrite',\n",
        "            \"WQFM_WALK2\" : 'QFM',\n",
        "            \"WSID_WALK2\" : 'Siderite'}\n",
        "\n",
        "#Use Only Columns Have Detections\n",
        "pn_usable = list(pn_cols.keys())\n",
        "for col in pn_cols.keys():\n",
        "    min_s = las_df[col].describe()['min']\n",
        "    prec_detect = round(las_df[las_df[col] > min_s].shape[0]/las_df.shape[0]*100,0)\n",
        "    if prec_detect <= 2:\n",
        "        pn_usable.remove(col)\n",
        "\n",
        "print(', '.join([pn_cols[x] for x in pn_usable]), 'found in >2% of samples.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSbEe1y5ZrG-"
      },
      "source": [
        "#Drop Null Values\n",
        "subset = list(curves.keys()) + list(pn_usable)\n",
        "las_df.dropna(subset=subset, inplace=True)\n",
        "\n",
        "#Seperate Our Training & Outcome Data\n",
        "X = las_df[curves.keys()]\n",
        "y = las_df[pn_usable]\n",
        "\n",
        "#Split off 10% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "#Set up our model; no randomness in bootstrapping with depth & trees counts set low for demo purposes\n",
        "regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=20)\n",
        "\n",
        "#Train our model\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "#Test outcome\n",
        "print('Model Score:',round(regr.score(X_test, y_test),3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLOg6xYKZrHA"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Think of this as tweaking the inputs of our algorithm to enhance performance.  One of the first things we will do before repeditavely running a model is to decrease the number of parameters by choosing those with the most influence on the outcome of our model. This allows us to run multiple iterations faster to demonstrate visually what happens as we adjust our model inputs while minimizing the impact on score.  We do risk eliminating a minor predictor feature by doing this but it's contribution to accuracy will be minor.\n",
        "\n",
        "The ultimate goal is to minimize error.  If our model is two simple we will not capture details present in the training data that represents the real world.  If we make our model too complex, or overfit, changes in our training data introduce error.  To decrease the irreducable error once the prior two are optimized we must have a more represenative dataset of the real world. Usually this means using a larger training dataset but eleminating sampling bias is also important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHNmokJZrHB"
      },
      "source": [
        "#Dropping non-contributers\n",
        "feats = []\n",
        "labels = []\n",
        "for score, mnemonic in zip(regr.feature_importances_, curves.keys()):\n",
        "    #Filter out curves that did not contribute first pass\n",
        "    if score == 0:\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        feats.append(score)\n",
        "        labels.append(mnemonic)\n",
        "\n",
        "##Visualize Those Curves' Contribution to Model\n",
        "explode = [0 if x > 0.03 else 5*(1-x)**100 for x  in feats]\n",
        "plt.pie(feats,labels=labels,explode=explode)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcAjuW4JZrHC"
      },
      "source": [
        "So with just filtering the inputs to our model to those that contributed on the first pass how does that affect the score?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPbWhfKbZrHD"
      },
      "source": [
        "#Drop Null Values\n",
        "subset = labels + pn_usable\n",
        "las_df.dropna(subset=subset, inplace=True)\n",
        "\n",
        "X = las_df[labels]\n",
        "y = las_df[pn_cols.keys()]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=20)\n",
        "regr.fit(X_train, y_train)\n",
        "print('Model Score:',round(regr.score(X_test, y_test),3))\n",
        "\n",
        "##Visualize Those Curves' Contribution to Model\n",
        "explode = [0 if x > 0.03 else 5*(1-x)**100 for x  in regr.feature_importances_]\n",
        "plt.pie(regr.feature_importances_,labels=labels,explode=explode)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwM0VEAsZrHG"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Number of Trees\n",
        "Now let's look at the affect of changing `n_estimators` or the number of trees in our forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMdUKrEZrHH"
      },
      "source": [
        "#muting warnings from here on out\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "scores = []\n",
        "times = []\n",
        "estimators = [10,50,100,200,500,1000,2000]#,5000,10000] #Can add some serious time.\n",
        "for esimator in estimators:\n",
        "    print('Training with',esimator)\n",
        "    t1 = time.time()\n",
        "    regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=esimator)\n",
        "    regr.fit(X_train, y_train)\n",
        "    scores.append(regr.score(X_test, y_test))\n",
        "    times.append(time.time() - t1)\n",
        "\n",
        "#Plot Results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Estimators')\n",
        "ax1.set_ylabel('Score', color='b')\n",
        "ax1.plot(estimators, scores, color='b')\n",
        "ax1.tick_params(axis='y', labelcolor='b')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('time (s)', color='r')\n",
        "ax2.scatter(estimators, times, color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPgu8ySjZrHI"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Depth of Trees\n",
        "\n",
        "\n",
        "Now let's look at the affect of changing `max_depth` or the maximum depth of the trees in our forest. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ni8Qcd4ZrHJ"
      },
      "source": [
        "scores = []\n",
        "times = []\n",
        "depths = [2,5,10,20,30,40,None]\n",
        "for depth in depths:\n",
        "    print('Training with',depth)\n",
        "    t1 = time.time()\n",
        "    regr = RandomForestRegressor(max_depth=depth, random_state=0, n_estimators=10)\n",
        "    regr.fit(X_train, y_train)\n",
        "    scores.append(regr.score(X_test, y_test))\n",
        "    times.append(time.time() - t1)\n",
        "\n",
        "#Replace None for purpose of plot\n",
        "depths = [x if x != None else 50 for x in depths ]\n",
        "\n",
        "#Plot Results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Depth')\n",
        "ax1.set_ylabel('Score', color='g')\n",
        "ax1.plot(depths, scores, color='g')\n",
        "ax1.tick_params(axis='y', labelcolor='g')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('time (s)', color='r')\n",
        "ax2.scatter(depths, times, color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-236uUeZrHK"
      },
      "source": [
        "---\n",
        "\n",
        "## Random State\n",
        "\n",
        "\n",
        "`random_state` controls two items in a random forest model: the randomness of the samples that each tree gets with boostraping & the selection of features to consider when looking to split at each node. `None`(default) uses `np.random` but if we want our model to produce the same result when runnning multiple times (ie deterministic) we can use an integer to seed a random number generator. `0` and `42` are commonly used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlVbnoZkZrHL"
      },
      "source": [
        "scores = []\n",
        "times = []\n",
        "randomness = [0,3,7,41,42,43,None,None,None,None,None,None]\n",
        "for rand in randomness:\n",
        "    print('Training with',rand)\n",
        "    t1 = time.time()\n",
        "    regr = RandomForestRegressor(max_depth=2, random_state=rand, n_estimators=10)\n",
        "    regr.fit(X_train, y_train)\n",
        "    scores.append(regr.score(X_test, y_test))\n",
        "    times.append(time.time() - t1)\n",
        "\n",
        "#Replace None for purpose of plot\n",
        "randomness = [x if x != None else 100+i for i,x in enumerate(randomness)]\n",
        "\n",
        "#Plot Results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Random State')\n",
        "ax1.set_ylabel('Score', color='g')\n",
        "ax1.plot(randomness, scores, color='g')\n",
        "ax1.tick_params(axis='y', labelcolor='g')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('time (s)', color='r')\n",
        "ax2.scatter(randomness, times, color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hQ442vFZrHN"
      },
      "source": [
        "Notice the changes in score? What happens when we do not seed the random number generator?\n",
        "\n",
        "---\n",
        "\n",
        "## Max Features\n",
        "`max_features` or the maximum number of features considered for splitting a node. This parameter also take special case strings: `'auto'`(default),`'sqrt'`, `'log2'`, `None`(default).  Generally, by having \"dumb\" trees you increase the variety of trees allowing for different features to show through the dominant features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeoqXt5aZrHN"
      },
      "source": [
        "scores = []\n",
        "times = []\n",
        "max_features = [2,5,10,X.shape[1],'sqrt','log2',None]\n",
        "for mx in max_features:\n",
        "    print('Training with',mx)\n",
        "    t1 = time.time()\n",
        "    regr = RandomForestRegressor(max_depth=2, max_features=mx,random_state=0, n_estimators=10)\n",
        "    regr.fit(X_train, y_train)\n",
        "    scores.append(regr.score(X_test, y_test))\n",
        "    times.append(time.time() - t1)\n",
        "\n",
        "#Replace Strings and None for purpose of plot\n",
        "max_features = [x if x != None else 28 for x in max_features]\n",
        "max_features = [x if x != 'log2' else 27 for x in max_features ]\n",
        "max_features = [x if x != 'sqrt' else 26 for x in max_features ]\n",
        "max_features = [x if x != 'auto' else 25 for x in max_features ]\n",
        "\n",
        "\n",
        "#Plot Results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Max Features')\n",
        "ax1.set_ylabel('Score', color='k')\n",
        "ax1.plot(max_features, scores, color='k')\n",
        "ax1.tick_params(axis='y', labelcolor='k')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('time (s)', color='r')\n",
        "ax2.scatter(max_features, times, color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V6n79A9ZrHP"
      },
      "source": [
        "What was the optimum maximum? Remember that 25-28 are the special cases.\n",
        "\n",
        "---\n",
        "## Min Features\n",
        "`min_samples_split` or the minimum number of samples required to split an internal node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAb75jrnZrHP"
      },
      "source": [
        "scores = []\n",
        "times = []\n",
        "min_features = [2,3,4,5]\n",
        "for mn in min_features:\n",
        "    print('Training with',mn)\n",
        "    t1 = time.time()\n",
        "    regr = RandomForestRegressor(max_depth=2, min_samples_split=mn, random_state=0, n_estimators=10)\n",
        "    regr.fit(X_train, y_train)\n",
        "    scores.append(regr.score(X_test, y_test))\n",
        "    times.append(time.time() - t1)\n",
        "\n",
        "#Replace Strings for purpose of plot\n",
        "min_features = [x if x != 'log2' else 27 for x in min_features ]\n",
        "min_features = [x if x != 'sqrt' else 26 for x in min_features ]\n",
        "min_features = [x if x != 'auto' else 25 for x in min_features ]\n",
        "\n",
        "\n",
        "#Plot Results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel('Min Features')\n",
        "ax1.set_ylabel('Score', color='grey')\n",
        "ax1.plot(min_features, scores, color='grey')\n",
        "ax1.tick_params(axis='y', labelcolor='grey')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.set_ylabel('time (s)', color='r')\n",
        "ax2.scatter(min_features, times, color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElETf-7NZrHR"
      },
      "source": [
        "In this case we see that changing the minimum number of features to split a node doesn't have a positive affect on our model.\n",
        "\n",
        "---\n",
        "\n",
        "These relationships are not always independent and you can also test them against each other to get the most out of your model.\n",
        "\n",
        " ### <font color='red'>NOTE</font>:\n",
        " *If you <font color='red'>are not</font> running a faster computer **skip** the cell below and **load** the data with the cell below it.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhjuYOmzZrHR"
      },
      "source": [
        "#********************************************RUN*************************************************#\n",
        "#To run it yourself run this cell\n",
        "depths = list(range(2,21))\n",
        "trees = list(range(10,2000,100))\n",
        "trees = [2,3,4,5,6,7,8,9,10,25,50,75,100,150,200,250,500,1000,1500,2000]\n",
        "\n",
        "\n",
        "xa = np.array([depths]*20).T\n",
        "ya = np.array([trees]*19)\n",
        "\n",
        "Z_s = []\n",
        "Z_t = []\n",
        "for depth in depths:\n",
        "    z_s = []\n",
        "    z_t = []\n",
        "    for tree in trees:\n",
        "        t1 = time.time()\n",
        "        regr = RandomForestRegressor(max_depth=depth, random_state=0, n_estimators=tree, n_jobs=-1)\n",
        "        regr.fit(X_train, y_train)\n",
        "        z_t.append(time.time() - t1)\n",
        "        z_s.append(regr.score(X_test, y_test))\n",
        "    Z_s.append(z_s)\n",
        "    Z_t.append(z_t)\n",
        "\n",
        "Z_t = np.array(Z_t)\n",
        "Z_s = np.array(Z_s)\n",
        "#********************************************RUN*************************************************#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VDo4Yq0ZrHT"
      },
      "source": [
        "#*******************************************LOAD************************************************#\n",
        "\n",
        "#To load the pre-completed data from the cell above run this cell\n",
        "depths = list(range(2,21))\n",
        "trees = [2,3,4,5,6,7,8,9,10,25,50,75,100,150,200,250,500,1000,1500,2000]\n",
        "\n",
        "xa = np.array([depths]*20).T\n",
        "ya = np.array([trees]*19)\n",
        "\n",
        "Z_t = np.loadtxt(\"https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/Z_t.out\")\n",
        "Z_s = np.loadtxt(\"https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/Z_s.out\")\n",
        "#*******************************************LOAD************************************************#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qai7THhXFzOS"
      },
      "source": [
        "##Plot the number and depth of trees with the model score and amount of time.\n",
        "#Make Figure with 2 3D Subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n",
        "\n",
        "#Add Subplot 1\n",
        "fig.add_trace(\n",
        "              go.Surface(x=xa,\n",
        "                        y=ya,\n",
        "                        z=Z_s,\n",
        "                        colorscale='Viridis',\n",
        "                        showscale=False,\n",
        "                        contours = {\"z\": {\"show\": True,\n",
        "                                          \"start\": 0.1,\n",
        "                                          \"end\": 0.9,\n",
        "                                          \"size\": 0.025,\n",
        "                                          'color':'black'}\n",
        "                                    },\n",
        "                        name='Score',\n",
        "                        hovertemplate ='%{y} trees with a depth of %{x}'+\n",
        "                                        ' yeilds a score of %{z: .2f}.'\n",
        "                        ),\n",
        "              row=1,\n",
        "              col=1\n",
        "              )\n",
        "\n",
        "#Add Subplot 2\n",
        "fig.add_trace(\n",
        "              go.Surface(x=xa,\n",
        "                        y=ya,\n",
        "                        z=Z_t,\n",
        "                        colorscale='RdBu',\n",
        "                        showscale=False,\n",
        "                        name='Compute Time',\n",
        "                        contours = {\"z\": {\"show\": True,\n",
        "                                          \"start\": 1,\n",
        "                                          \"end\": 10,\n",
        "                                          \"size\": 0.5}\n",
        "                                    },\n",
        "\n",
        "                        hovertemplate ='%{y} trees with a depth of %{x}'+\n",
        "                                       ' yeilds a compute time of %{z: .1f}.'\n",
        "                        ),\n",
        "\n",
        "              row=1,\n",
        "              col=2)\n",
        "\n",
        "#Adjust Size and Set Title\n",
        "fig.update_layout(\n",
        "                  title_text='Number and Depth of Trees Effect on Score & Compute Cost',\n",
        "                  height=600,\n",
        "                  width=1500,\n",
        "                 )\n",
        "\n",
        "#Setup Default Initial View\n",
        "camera = dict(\n",
        "              up=dict(x=0, y=0, z=1),\n",
        "              center=dict(x=0, y=0, z=0),\n",
        "              eye=dict(x=-1.75, y=-1.75, z=0.25)\n",
        "          )\n",
        "\n",
        "#Change Axis Labels on Subplot 1\n",
        "fig.update_layout(scene1 = dict(xaxis_title='Tree Depth',\n",
        "                                yaxis_title='N Trees',\n",
        "                                zaxis_title='Score',\n",
        "                                camera=camera)\n",
        "                  )\n",
        "\n",
        "#Change Axis Labels on Subplot 2\n",
        "fig.update_layout(scene2 = dict(xaxis_title='Tree Depth',\n",
        "                                yaxis_title='N Trees',\n",
        "                                zaxis_title='Time (ms)',\n",
        "                                camera=camera),\n",
        "                  )\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ4xSKcZZrHY"
      },
      "source": [
        "Drag points on the models to rotate them.\n",
        "- Notice the plateau in the score that we reach around 250 trees?  \n",
        "- Notice that the influence of the depth of the trees decreases with the increaded number of trees?\n",
        "\n",
        "---\n",
        "\n",
        "# Give it a try\n",
        "\n",
        "With what we learned let's retrain our model. Pick inputs from what we learned above and retrain the model.  \n",
        "- Adjust and try it again.\n",
        "- Better or worse score?\n",
        "- Was there a decrease or increase of training time?\n",
        "\n",
        "By setting `n_jobs` to `-1` we can speed this up by using all processors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xJiCcFZrHY"
      },
      "source": [
        "subset = labels + pn_usable\n",
        "\n",
        "las_df.dropna(subset=subset, inplace=True)\n",
        "\n",
        "X = las_df[labels]\n",
        "y = las_df[pn_usable]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "##Enter your values\n",
        "regr = RandomForestRegressor(max_depth=2 ,\n",
        "                             n_estimators=20 ,\n",
        "                             max_features='sqrt' ,\n",
        "                             random_state=42 ,\n",
        "                             n_jobs=-1)\n",
        "\n",
        "regr.fit(X_train, y_train)\n",
        "print('Model Score:',round(regr.score(X_test, y_test),3),'in',round(time.time() - t1,3),'sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiC82BP0ZrHa"
      },
      "source": [
        "So we've just about doubled our model's score showing the importance of hyperameter tuning.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Saving & Loading\n",
        "\n",
        "So you want to deploy your model but don't want to train it every time?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcrzkjvOZrHa"
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "#Saving Model\n",
        "dump(regr, 'regr.joblib')\n",
        "\n",
        "#Loading Model\n",
        "regr2 = load('regr.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPCrgaV1ZrHc"
      },
      "source": [
        "Let's take a look at how you would deploy it to make predictions. Let's load a log that the model hasn't seen before and see how it does..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8AcyT81ZrHc"
      },
      "source": [
        "#Load\n",
        "las2 = lasio.read(\"https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/0512338763_3599412.las.txt\")\n",
        "las_df2 = las2.df()\n",
        "\n",
        "#Drop Null Values\n",
        "subset = labels + pn_usable\n",
        "las_df2.dropna(subset=subset, inplace=True)\n",
        "\n",
        "#Run Model\n",
        "X = las_df2[labels][las_df2[labels].notnull()]\n",
        "y_pred = regr2.predict(X)\n",
        "\n",
        "#Write Predictions to the DF\n",
        "new_cols = [x.split('_')[0] for x in pn_usable]\n",
        "\n",
        "for i,col in enumerate(new_cols):\n",
        "    las_df2[col] = y_pred[:,i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tC12yxxZrHe"
      },
      "source": [
        "#Plot the results\n",
        "c = ['blue','brown','goldenrod','gold']\n",
        "x = las_df2\n",
        "ztop=x.index.min(); zbot=x.index.max()\n",
        "\n",
        "f, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 13))\n",
        "for i,col in enumerate(new_cols):\n",
        "    ax[i].set_ylim(ztop,zbot)\n",
        "    ax[i].plot(x[col+'_WALK2'],x.index, color=c[i], alpha=0.3)\n",
        "    ax[i].plot(x[col],x.index,color='grey')\n",
        "    ax[i].set_title(col)\n",
        "    ax[i].invert_yaxis()\n",
        "    ax[i].grid(False)\n",
        "    if i !=0:\n",
        "        ax[i].set_yticklabels([])\n",
        "ax[0].set_ylabel('MD [ft]', fontsize=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFnzzjhOZrHg"
      },
      "source": [
        "- Where is your model working?\n",
        "- Where is it not?\n",
        "- What could be done to improve the model?\n",
        "- How do you think changes in fluids affects the model?"
      ]
    }
  ]
}