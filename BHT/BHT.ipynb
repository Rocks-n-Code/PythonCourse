{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    " Copyright (c) 2020 Matthew W. Bauer, P.G.\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " of this software and associated documentation files (the \"Software\"), to deal\n",
    " in the Software without restriction, including without limitation the rights\n",
    " to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " copies of the Software, and to permit persons to whom the Software is\n",
    " furnished to do so, subject to the following conditions:\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in all\n",
    " copies or substantial portions of the Software.\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    " SOFTWARE.\n",
    "___\n",
    "\n",
    "Source: https://github.com/Rocks-n-Code/PythonCourse/\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Python to Improve Your Understanding of Geologic Conditions in the Rockies:\n",
    "## Bottom Hole Temperature Workflow\n",
    "\n",
    "By Matthew W. Bauer, P.G.\n",
    "\n",
    "How much does your team spend in time and money gathering datasets, fixing errors, and relating multiple data sources before you can interpret? Are there conventional workflows that the time needed to apply them prevents their application over regional areas? What about datasets that the variable and sample counts are so large that it is hard to wrap your mind around? Have you tried to understand the economics and risk of a project without having absolute inputs?\n",
    "\n",
    "Adding python programming to your workflows can help with all of that. It isn’t a magic bullet so understanding what it can and can’t do is important. Workflow automation and machine learning can't replace the domain expertise, abstract thought, or creativity of a good geologist. That said coding literacy provides large benefits to earth scientists by being able to acquire and utilize large datasets efficiently. I also argue that those benefits can start to be realized earlier in the learning process than the traditional “10,000-hour” learning threshold. Especially time savings in accessing and cleaning data. So what packages should you start learning?\n",
    "\n",
    "The python library `pandas` allows us to efficiently open a larger number of file formats in larger sizes than Excel can handle. Finding and fixing errors in datasets with boolean filters and regular expressions can eliminate the days of sifting through an Excel workbook fixing issues by hand. The ease and speed in which higher-level math can be applied to these multidimensional arrays can simplify the multiple columns and complex nested functions required in Excel. Merging datasets in `pandas` greatly simplifies the sometimes tedious vlookup process. The fuzzy string matching capabilities of the library `fuzzywuzzy` combined with `pandas` can easily make looking up missing API numbers from well names with slight variations in spelling a thing of the past. The python libraries `lasio` and `welly` allow us to open, interact with, and save LAS logs. Limitations on time and data budgets can limit the number of well logs, tops, and downhole testing that we base our interpretations on and, potentially, increase a project's risk. Automating the process of accessing public data, also known as scraping, with the libraries `requests`, `selenium`, and `urllib` can increase the amount of data we can access and vastly improve our understanding of natural systems.\n",
    "\n",
    "### Bottom Hole Temperatures\n",
    "Let's take a look at an example of an automated workflow for LAS logs. Whether you're mapping geothermal prospects, or building maturity models, bottom hole temperature (BHT) data is essential. Some G&G software packages allow parsing of LAS file headers otherwise you are left reviewing one file at a time. Parsing can be automated making the processing of approximately 64k files on a solid-state drive for BHT data achievable overnight. First, we will import the libraries that we will be using and set some variables for working in a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd     #library for working with dataframes\n",
    "import geopandas as gpd #library for working with geospatial data\n",
    "import lasio            #library for working with LAS files\n",
    "import glob             #library for finding files\n",
    "import os               #library for interacting with files\n",
    "import re               #library for regular expressions\n",
    "\n",
    "#Set options\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the library `glob` we can search for our files with the file path and a wildcard character of \"*\". For Windows users, once we have the list of files then we’ll make the slash direction uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find your LAS files\n",
    "las_path = \"D:/CO/CO_LAS/*.las\"\n",
    "files = [x.replace('\\\\','/') for x in glob.glob(las_path)]\n",
    "print(len(files),'LAS files found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will loop through those files and use the library `lasio` to open each file. We will then pull the LAS file header parameters into a temporary `pandas` dataframe and filter to aliases of temperature readings in the header info and collect the highest reading. We can also collect the maximum measured depth from each log. We'll add this, or concatenate, to our out primary dataframe that we'll save out occasionally and once we are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_alias = ['BHT','BHT:1','BHT:2','BHT:3','BHT:4',\n",
    "    'MRT','MRT:1','MRT:2','MRT2:1',\n",
    "    'MRT1:1','MRT2:2','MRT1:2','MRT:3',\n",
    "    'MRT2','MRT3','MRT4','MRT1',\n",
    "    'MRT                         192',\n",
    "    'MAXRECTEMP',\n",
    "    'BOTTEMP','BOTTOMHOLETEMP','TEMP','TEMPERATURE','MAXTEMP',\n",
    "    'BHTEMP','BHTEMP:1','BHTEMP:2','BHTEMP_SRC']\n",
    "\n",
    "celcius_units = ['degC','DEGC']\n",
    "\n",
    "depth_alias = ['DEPT','DEPTH','M_DEPTH','DPTH','DEPT:1','MD','DEPTH:1','DEPTH_HOLE','BDEP',\n",
    "    'DMEA','TOTAL_DEPTH','TVD','DEP','TDEP','DEPTMEAS','DEPT_PNN','DEPT_CBL',\n",
    "    '\"DEPTH\"','TVD:1','DEP:1']\n",
    "\n",
    "#Make empty list and dataframe\n",
    "err_files = []\n",
    "bht_df = pd.DataFrame()\n",
    "\n",
    "#Place files that cause hangups here\n",
    "hangups = []\n",
    "\n",
    "#Saveout counter\n",
    "i = 0\n",
    "\n",
    "#Loop through files\n",
    "for file in files:\n",
    "    if file in hangups: continue\n",
    "    i += 1\n",
    "    try:\n",
    "        las = lasio.read(file)\n",
    "        params = {'mnemonic' : [x.mnemonic for x in las.params],\n",
    "                  'unit' : [x.unit for x in las.params],\n",
    "                  'value' : [x.value for x in las.params],\n",
    "                  'descr' : [x.descr for x in las.params]}\n",
    "\n",
    "        temp = pd.DataFrame(params)\n",
    "        temp['API'] = file.split('/')[-1].split('_')[0]\n",
    "        temp['file'] = file.split('/')[-1]\n",
    "        \n",
    "        #Pull Maximum Depth\n",
    "        depths = [x.mnemonic for x in las.curves if x.mnemonic in depth_alias]\n",
    "        max_depth = max([las[x].max() for x in depths])\n",
    "        temp['MaxMD'] = max_depth\n",
    "\n",
    "        #Filter to just temperature alisas\n",
    "        temp = temp[temp.mnemonic.isin(temp_alias)]\n",
    "        \n",
    "        bht_df = pd.concat([bht_df,temp],ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(file,e)\n",
    "        err_files.append(file)\n",
    "        \n",
    "    if i > 100:\n",
    "        i = 0\n",
    "        print(round(files.index(file)/len(files)*100,2),'%complete')\n",
    "        bht_df.to_csv('bht_df.csv',index=False)\n",
    "\n",
    "        \n",
    "bht_df.to_csv('bht_df.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the BHT data parsed from the LAS files we will need to clean it up to make it usable. The values contain non-number characters such as \"°\" or \" F\" that we will need to remove. Regular expressions, or `re`, can do this easily in a single line of code. Regular expressions do this by expanding our search capabilities to ranges of characters or even patterns of characters. In this case, we'll use `re` to search for anything that isn't a number or a decimal place and remove those characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_count = bht_df.shape[0]\n",
    "\n",
    "#Drop Null Values\n",
    "bht_df = bht_df[bht_df.value.notnull()]\n",
    "\n",
    "#Remove non-number characters other than \".\"\n",
    "bht_df.value = bht_df.value.apply(lambda x: re.sub(\"[^0-9.]\",\"\",x))\n",
    "\n",
    "#Drop empty values\n",
    "bht_df = bht_df[bht_df.value != '']\n",
    "\n",
    "#Drop multiple \".\" Tool IP addresses?\n",
    "bht_df = bht_df[bht_df.value.apply(lambda x: str(x).count('.') <= 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now change the variable type from strings into float numbers. Once the data is actually numbers we can convert the Celsius values to Fahrenheit and change the unit label. Doing this with `where` allows us to convert units in select locations by using a boolean that is only false where we want to change the data in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bht_df['value'] = bht_df['value'].astype(float)\n",
    "\n",
    "for C_col in ['DEGC','degC']:\n",
    "    bht_df['value'] = bht_df['value'].where(bht_df['unit'] != C_col,\n",
    "                                            other=bht_df['value'].apply(lambda x: (9/5)*x + 32))\n",
    "    \n",
    "    bht_df['unit'] = bht_df['unit'].where(bht_df['unit'] != C_col,\n",
    "                                           other='degF')\n",
    "#Drop Bad Values\n",
    "bht_df = bht_df[bht_df.value >= 0]\n",
    "bht_df = bht_df[bht_df.value < 500]\n",
    "\n",
    "print(pre_count - bht_df.shape[0],'of',pre_count,'rows dropped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Datasets\n",
    "\n",
    "Merging large datasets in excel usually involves ordering the lookup table and using 'vlookup' to pull the column we want. We are going to use `geopandas` to open a shapefile and manipulate it with the same syntax as `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = gpd.read_file('./WellSpot/Wells.shp')\n",
    "wells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge depth and location data from the `well` geodataframe to the `bht_df` dataframe we will need a common column. To achieve this we'll format the API number in `bht_df` to create a new column, “API_Label”, that is the same format as `wells`. Applying a lambda function makes this easy and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make API_Label\n",
    "bht_df['API_Label'] = bht_df.API.apply(lambda x: x[:2] + '-' + x[2:5] + '-' + x[5:10])\n",
    "\n",
    "#Merge\n",
    "bht_df = bht_df.merge(wells[['API_Label','Max_MD','Max_TVD','geometry']],\n",
    "                      how='left',\n",
    "                      on='API_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Data for QA\n",
    "\n",
    "Pandas allows us to easily visualize our data in a scatter plot and colorize it with a property of our data. This makes it easier to identify potentially erroneous data points. We will calculate the count of each unique value using `value_counts` then plot setting the color to that property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Counts column\n",
    "vcounts = bht_df.value.value_counts()\n",
    "values = vcounts.keys().tolist()\n",
    "counts = vcounts.tolist()\n",
    "counts_dict = dict(zip(values, counts))\n",
    "bht_df['count'] = bht_df.value.apply(lambda x: counts_dict[x])\n",
    "\n",
    "#Plotting Values with TVD\n",
    "bht_df[bht_df.value < 400].plot.scatter(x='value',\n",
    "                                        y='Max_TVD',\n",
    "                                        c='count',\n",
    "                                        colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the values that are not following the depth trend. Whether defaults or \"pencil-whipped\", the values do not appear to be correct so let's remove them. We can do this easily with `~`, which means the opposite of, the `isin` boolean which checks for the presence of a value in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing suspect values\n",
    "bad_values = [212,211.99986,200,150,70,32,0]\n",
    "bht_df = bht_df[~bht_df['value'].isin(bad_values)]\n",
    "\n",
    "#Plot data\n",
    "bht_df.plot.scatter(x='value',\n",
    "                    y='Max_TVD',\n",
    "                    c='count',\n",
    "                    colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data\n",
    "Depending on how you'd like to use this data would dictate the file output type. Let's look at three of the most common. \n",
    "\n",
    "#### CSV\n",
    "CSVs allow us a lot of flexibility with their small size and wide range of programs that can open them. If you're a Petrel user you may need tab-delimited import data so let's save out that format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save out to csv\n",
    "bht_df.to_csv(\"CO_BHT.csv\", index=False)\n",
    "\n",
    "#Save out to tab-delimited\n",
    "bht_df.to_csv(\"CO_BHT_tab.csv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel\n",
    "Don't think that while I may rip on Excel that it doesn't have its place in data analysis. It is the standard software for working with data in most industries, makes nice plots, and has some decent tools built-in. Because of that excel is an excellent choice for sharing work with other people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bht_df.to_excel(\"CO_BHT.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GIS\n",
    "The power of preserving & being able to reference data's spatial location can not be understated. Geopandas allows us to convert to a geodataframe and save easily. It also makes managing CRS a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(bht_df, geometry='geometry',crs=wells.crs)\n",
    "\n",
    "#Save GeoDataFrame\n",
    "gdf.to_file('bht_gdf.shp')\n",
    "\n",
    "#Change CRS\n",
    "gdf.to_crs({'init':'EPSG:4326'},inplace=True)\n",
    "\n",
    "#Save Out Copy\n",
    "gdf.to_file('bht_gdf_WGS84.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this workflow, we've shown how to use python to parse a large number of LAS files for information data out of the header. While not yet corrected BHT you can already see regional trends. With that, you can identify areas of interest and make corrections from wells we have already identified with BHT data parsed from public LAS files.\n",
    "\n",
    "![BHT Data in Colorado](BHT.png)\n",
    "\n",
    "If you'd like this code in a Jupyter notebook, the data it produced, python lessons, or other workflows visit my GitHub at https://github.com/Rocks-n-Code/PythonCourse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
