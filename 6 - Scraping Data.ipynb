{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "6 - Scraping Data.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocks-n-Code/PythonCourse/blob/master/6%20-%20Scraping%20Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "qAEhUo-xhamR",
        "colab_type": "text"
      },
      "source": [
        "# Scraping Data\n",
        "\n",
        "I imagine not all of you are working for super majors and have access to every log or dataset known to man.  That also being said I don't think your boss is going to let you buy a thousand digital logs from *a vendor at $150 a pop for a regional study to support a prospect.  Your tech, if you have access to one, is also going to want to murder you if you ask them to go download files from the state one well at a time as well. To help with this lets use python to simulate a user interacting with a browser in a process know as scraping.\n",
        "\n",
        "The two styles of scraping that we'll touch on today: with and without a browser.  A third style uses a [web spider](https://scrapy.org/) but we won't get to that today.\n",
        "\n",
        "With scraping:\n",
        "-  Check terms of service from the website.\n",
        "-  Don't scrape agressively as you can cause enough traffic to affect other users. Be a Good Citizen! Don't be a dick. (ie Be Nice)\n",
        "-  Just plan on the website changing from time to time and having to re-write scrapers.\n",
        "\n",
        "So let's all take an oath...\n",
        "\n",
        "---\n",
        "\n",
        "## Scraping Without a Browser\n",
        "This is generally a much faster way of collecting data but it doesn't handle data sources that have used features to make it harder to scrape.  In this exercise will be using `geopandas` to get basic information, `requests` to fetch our data, parse that data, then we'll store it to a `.csv` with `pandas`.  We'll walk through how to parse text and **build** a scraper for public data for this example.  After we test it we'll roll it into its automated form with a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnuuvC0qh12A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi3lYRL7hamR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from numpy import nan\n",
        "import geopandas as gpd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_columns = 999\n",
        "pd.options.display.max_rows = 999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYVvBfZFhamU",
        "colab_type": "text"
      },
      "source": [
        "-  Open the `wells.shp` to a dataframe.\n",
        "-  Open COGCC's data portal in another tab in our browser. https://cogcc.state.co.us/data.html#/cogis\n",
        "-  Then navigate to \"facility\".\n",
        "\n",
        "Let's load in a dataframe of our Colorado wells and preview the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYbZl1L_hamV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Originally from COGCC well spot shapefile - Jackson County\n",
        "rawurl = 'https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/Jackson_057.csv'\n",
        "apis = pd.read_csv(rawurl)\n",
        "apis = gpd.GeoDataFrame(apis,\n",
        "                        crs='EPSG:26913')\n",
        "print(type(apis))\n",
        "apis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bx2cDdshamb",
        "colab_type": "text"
      },
      "source": [
        "-  Open [COGCC](https://cogcc.state.co.us/data.html#/cogis) in a new tab.\n",
        "-  On the [website](https://cogcc.state.co.us/data.html#/cogis) select WELL under facility type and select JACKSON county and search.\n",
        "-  Click on a few wells. Notice that the URL doesn't change.\n",
        "-  Now this time open a well in a new tab (Right click + 'Open link in new tab').\n",
        "-  Notice that the URL is now specific to that well.\n",
        "\n",
        "We're going to utilize this to get more information in a usable format for these wells.  Let's break out the non-unique portions of this URL to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3UP3bV7hamc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseURL = 'https://cogcc.state.co.us/cogis/FacilityDetail.asp?facid='\n",
        "tailURL = '&type=WELL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XcfKxZphamd",
        "colab_type": "text"
      },
      "source": [
        "Generally websites like this will have a base URL seperated by `?` followed by variables. Notice that COGCC doesn't use the state code in the API number with no deliminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OK6wAZThame",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = baseURL + '05-057-05128'.replace('-','')[2:] + tailURL\n",
        "print('URL:', url)\n",
        "r = requests.get(url)\n",
        "print('Encoding:', r.encoding)\n",
        "print('RespCode:',type(r.status_code),r.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUmQyhGGhamg",
        "colab_type": "text"
      },
      "source": [
        "A response code of `200` lets us know that it was a good request. No let's look at the text that COGCC sent us back..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xno7lH9Yhamg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Z-1UGqhamj",
        "colab_type": "text"
      },
      "source": [
        "Now look back at the [well website](https://cogcc.state.co.us/cogis/FacilityDetail.asp?facid=05705128&type=WELL). Look for a keyword towards the tops listed at the bottom of the site. Copy this keyword `Ctrl+C` move to this tab, open find `Ctrl+F` and scroll through till you find where the tops are. We are looking for unique sections of the string to split on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXji2HrKhamj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split out tops section\n",
        "tops = r.text.split('<!-- LOOP FOR EACH Formation WITHIN WELLBORE -->')[1].split('<!-- END Formation LOOP -->')[0]\n",
        "tops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiW9G8ithaml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove special characters\n",
        "tops = tops.replace('\\r','').replace('\\n','').replace('\\t','').strip()\n",
        "tops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsoNbUW9hamn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split individual tops\n",
        "tops = tops.split('<tr>')\n",
        "tops = [x.strip() for x in tops if len(x) > 0]\n",
        "tops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI3WuTkUhamo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clean up data\n",
        "for top in tops:\n",
        "    cols = top.split('<td')\n",
        "    cols = [x.strip() for x in cols if len(x) > 0]    \n",
        "    formation = cols[0].split('color=\"Navy\">')[1].split(' ')[0]\n",
        "    depth = cols[1].split('<font size=\"2\">')[1].split('</font>')[0]\n",
        "    print(formation,':',depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtcrQtDJhamq",
        "colab_type": "text"
      },
      "source": [
        "Ok now let's roll all that up into a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pf3rwmKhamq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_parse(text):\n",
        "    tops = text.split('<!-- LOOP FOR EACH Formation WITHIN WELLBORE -->')[1].split('<!-- END Formation LOOP -->')[0]\n",
        "    tops = tops.replace('\\r','').replace('\\n','').replace('\\t','').strip()\n",
        "    tops = tops.split('<tr>')\n",
        "    tops = [x.strip() for x in tops if len(x) > 0]\n",
        "    \n",
        "    formations = {}\n",
        "    for top in tops:\n",
        "        cols = top.split('<td')\n",
        "        cols = [x.strip() for x in cols if len(x) > 0]    \n",
        "        formation = cols[0].split('color=\"Navy\">')[1].split(' ')[0]\n",
        "        depth = eval(cols[1].split('<font size=\"2\">')[1].split('</font>')[0])\n",
        "        formations[formation] = depth\n",
        "    return formations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zuMpbjfhams",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_parse(r.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSxNXjChamu",
        "colab_type": "text"
      },
      "source": [
        "And itterate through our wells. It is _EXTREMELY_ important to add `try` `except` to handle errors in scraping. Scrapers deal with others people's code and things *will* go wrong. It's also a good idea on long scrapes to periodically saveout your progress as there's nothing worse then getting back to something that ran all weeekend pulling data that you need for a project and to see that it crashed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrxo9Z70hamu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topDF = pd.DataFrame()\n",
        "i = 0\n",
        "apiSample = apis.head(10) #We'll only do the first few for this example \n",
        "total = apiSample.shape[0]\n",
        "\n",
        "for index, row in apiSample.iterrows(): \n",
        "    i += 1\n",
        "    prec = str(int(100*i/total)) + '% complete  '\n",
        "    print(row['API_Label'], prec, end='\\r')\n",
        "    try:\n",
        "        url = baseURL+row['API_Label'].replace('-','')[2:]+tailURL\n",
        "        r = requests.get(url)\n",
        "\n",
        "        if r.status_code == 200:\n",
        "            formations = top_parse(r.text)\n",
        "            formations['API'] = row['API_Label']\n",
        "            topDF = topDF.append(formations,ignore_index=True)\n",
        "            time.sleep(5) #Wait 5 sec.\n",
        "        else:\n",
        "            print(row['API_Label'],':',r.status_code)\n",
        "    except Exception as e:\n",
        "        print('Error:',row['API_Label'],e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-XhEwEvhamw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topDF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWXDLbD5hamx",
        "colab_type": "text"
      },
      "source": [
        "I've gone ahead and pulled all the tops for Jackson County for you.  This took approximately an hour and a half for 771 records to give you an idea of the time needed. These are avalible in the project folder.  This was a basic example with `requests` but if this is something you would like to do regularly I suggest you also check out `urllib`.  There are packages avalible to make the searching and parsing of the html much easier but when you're troubleshooting a tough website it's good to know what you are looking for\n",
        "\n",
        "---\n",
        "\n",
        "# Scraping with a Browser with Selenium\n",
        "\n",
        "Scraping with a browser allows you to navigate around obsticles that are often put in place to discourage scraping, fillout forms, and interact with a website in ways that `requests` can't.  That being said it can be significantly more challenging and can sometimes take much longer. In this example we will pull production data from COGCC. `selenium` locates \"elements\" of a web page to interact with them to preform tasks. There are several [different methods](https://selenium-python.readthedocs.io/locating-elements.html) to locate elements. We will also use `bs4` to parse a table from html. BeautifulSoup uses tag names and daughter relationships to make finding data easier.  \n",
        "\n",
        "I've previously written up this function but please open COGCC's [facility search](https://cogcc.state.co.us/cogis/FacilitySearch.asp) in a new tab. Select \"Well\", enter Weld County's code \"123\", and the sequence code \"39340\". Hit search. Select the well that comes up. Note the URL.\n",
        "\n",
        "With that open, copy the link from the well name.  Notice that there is one of these per wellbore. Paste this url into a new tab. Now let's walk through finding elements & using tags to find the data you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlRbGUQIrbh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To selenium Run in Colab\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "#Set Options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "#Make Web Driver\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "##For Jupyter Notebooks\n",
        "#from selenium import webdriver\n",
        "\n",
        "##Make Driver\n",
        "#chromedriver = \"chromedriver.exe\" #Path to your chromedriver - https://sites.google.com/a/chromium.org/chromedriver/\n",
        "#driver = webdriver.Chrome(executable_path=chromedriver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QyLo0bQhamy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.options.display.max_columns = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "w3HE0SfBhamz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pull_CO_prod(api_05, df, driver, pull_excel=False):\n",
        "    url = 'https://cogcc.state.co.us/cogis/FacilityDetail.asp?facid='+api_05+'&type=WELL'\n",
        "    print(url)\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    links = [driver.find_elements_by_tag_name('a')]\n",
        "    prod_wellbores = [x.get_attribute(\"href\") for x in driver.find_elements_by_tag_name('a') if 'production' in x.get_attribute(\"href\")]\n",
        "    print('prod_wellbores',prod_wellbores)\n",
        "    for wellbore in prod_wellbores:\n",
        "        driver.get(wellbore)\n",
        "        time.sleep(1)\n",
        "        \n",
        "        #Download the file\n",
        "        if pull_excel:\n",
        "            dwnExcel = driver.find_element_by_xpath('//*[@id=\"mainContent_btnExport\"]')\n",
        "            #//*[@id=\"mainContent_btnExport\"]\n",
        "            dwnExcel.click()\n",
        "            \n",
        "        #Table HTML\n",
        "        table = driver.find_element_by_xpath('//*[@id=\"mainContent_pnlResults\"]/div')\n",
        "\n",
        "        #BeautifulSoup\n",
        "        soup = BeautifulSoup(table.get_attribute('innerHTML'), \"html.parser\")\n",
        "        \n",
        "        rows = soup.find_all('tr')\n",
        "        row_list = []\n",
        "        \n",
        "        #Pull Header \n",
        "        for tr in rows[:1]:\n",
        "            th = tr.find_all('th')\n",
        "            row = [i.text for i in th]\n",
        "            row_list.append(row)\n",
        "\n",
        "        #Pull Rows\n",
        "        for tr in rows[1:]:\n",
        "            td = tr.find_all('td')\n",
        "            row = [i.text.replace('\\xa0','') for i in td]\n",
        "            row_list.append(row)\n",
        "        \n",
        "        temp = pd.DataFrame(row_list[1:],columns=row_list[0])\n",
        "        temp['First of Month'] = pd.to_datetime(temp['First of Month'])\n",
        "        temp.sort_values(by='First of Month',inplace=True)\n",
        "\n",
        "        df = pd.concat([df,temp],ignore_index=True)\n",
        "\n",
        "        return df, driver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZvqPo3mham2",
        "colab_type": "text"
      },
      "source": [
        "# Give it a try\n",
        "\n",
        "Now that we have the function complete the `for` loop below to feed the individual apis, minus the state code, to the function. Remember that you need to pass the dataframe and the driver to the function too.\n",
        "\n",
        "Run it for the following wells: `0512339340`,`0512339383`,`0512339370`, & `0512339384`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM-enR-8SL8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##I've laid out the format for you below. Make edits at *1, *2, & *3.\n",
        "\n",
        "apis = ['0512339340','0512339383','0512339370', '0512339384'] #*1: Make a list of your UWI codes\n",
        "\n",
        "df = pd.DataFrame()#*2: Make an Empty DataFrame\n",
        "\n",
        "for api in apis:\n",
        "    \n",
        "    api_05 = api[2:]\n",
        "    print(api_05)\n",
        "    df, drive = pull_CO_prod(api_05, df, driver) #*3: Insert the function w/ inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHP8kRAVr3Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##I've laid out the format for you below. Make edits at *1, *2, & *3.\n",
        "\n",
        "apis = [] #*1: Make a list of your UWI codes\n",
        "\n",
        "df = #*2: Make an Empty DataFrame\n",
        "\n",
        "for api in apis:\n",
        "    \n",
        "    api_05 = api[2:]\n",
        "    print(api_05)\n",
        "    df, drive = #*3: Insert the function w/ inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMz6Sgqd0j-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set data types & preview data\n",
        "cols = ['Oil Produced','Gas Produced','Water Volume','Days Produced']\n",
        "for col in cols:\n",
        "  df[col].replace('',0,inplace=True)\n",
        "  df[col] = df[col].astype(float)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sTv0ZJxOEDf",
        "colab_type": "text"
      },
      "source": [
        "Plot cumulative oil curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl418qCGwZ82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig=plt.figure(figsize=(15, 5))\n",
        "ax=fig.add_subplot(111)\n",
        "\n",
        "for api_05, group in df[['First of Month','API Sequence','Days Produced','Oil Produced']].groupby('API Sequence'):\n",
        "  group['CumOil'] = group['Oil Produced'].fillna(0).cumsum()\n",
        "  \n",
        "  group['Days Produced'] = group['Days Produced'].replace('',0).astype(float)\n",
        "  \n",
        "  group['Total_Days'] = group['Days Produced'].cumsum()\n",
        "  \n",
        "  prod_start = df['First of Month'].min()\n",
        "  group['Elapsed_Days'] = group['First of Month'].apply(lambda x: (x - prod_start).days )\n",
        "  group['Elapsed_Days'] = group['Elapsed_Days'].astype(float)\n",
        "  ax.plot(group.Total_Days,\n",
        "          group.CumOil,\n",
        "          ls='-',\n",
        "          label=api_05,\n",
        "          fillstyle='none')\n",
        "\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}