{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "6 - Scraping Data.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocks-n-Code/PythonCourse/blob/master/6%20-%20Scraping%20Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "qAEhUo-xhamR"
      },
      "source": [
        "I covered this notebook for Denver Data Drivers and you can follow along with the video [here.](https://www.youtube.com/watch?v=cO8fWCPp_6k)\n",
        "\n",
        "---\n",
        "\n",
        "# Scraping Data\n",
        "\n",
        "I imagine not all of you are working for super majors and have access to every log or dataset known to man.  That also being said I don't think your boss is going to let you buy a thousand digital logs from *a vendor at $150 a pop for a regional study to support a prospect.  Your tech, if you have access to one, is also going to want to murder you if you ask them to go download files from the state one well at a time as well. To help with this lets use python to simulate a user interacting with a browser in a process know as scraping.\n",
        "\n",
        "The two styles of scraping that we'll touch on today: with and without a browser.  A third style uses a [web spider](https://scrapy.org/) but we won't get to that today.\n",
        "\n",
        "With scraping:\n",
        "-  Check terms of service from the website.\n",
        "-  Don't scrape agressively as you can cause enough traffic to affect other users. Be a Good Citizen! Don't be a dick. (ie Be Nice)\n",
        "-  Just plan on the website changing from time to time and having to re-write scrapers.\n",
        "\n",
        "So let's all take an oath...\n",
        "\n",
        "---\n",
        "\n",
        "## Scraping Without a Browser\n",
        "This is generally a much faster way of collecting data but it doesn't handle data sources that have used features to make it harder to scrape.  In this exercise will be using `geopandas` to get basic information, `requests` to fetch our data, parse that data, then we'll store it to a `.csv` with `pandas`.  We'll walk through how to parse text and **build** a scraper for public data for this example.  After we test it we'll roll it into its automated form with a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnuuvC0qh12A"
      },
      "source": [
        "# To install packages in the Colab instance that are not normally avalible run a\n",
        "# command line command with \"!\"\n",
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi3lYRL7hamR"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from numpy import nan\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_columns = 999\n",
        "pd.options.display.max_rows = 999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYVvBfZFhamU"
      },
      "source": [
        "-  Open the `wells.shp` to a dataframe.\n",
        "-  Open COGCC's data portal in another tab in our browser. https://cogcc.state.co.us/data.html#/cogis\n",
        "-  Then navigate to \"facility\".\n",
        "\n",
        "Let's load in a dataframe of our Colorado wells and preview the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYbZl1L_hamV"
      },
      "source": [
        "#Originally from COGCC well spot shapefile - Jackson County\n",
        "rawurl = 'https://raw.githubusercontent.com/Rocks-n-Code/PythonCourse/master/data/Jackson_057.csv'\n",
        "apis = pd.read_csv(rawurl)\n",
        "\n",
        "#Fix raw csv geometry column\n",
        "def str_to_point(point_string):\n",
        "  x = int(point_string.split('(')[1].split(' ')[0])\n",
        "  y = int(point_string.split('(')[1].split(' ')[1].replace(')',''))\n",
        "  return Point(x,y)\n",
        "\n",
        "apis['geometry'] = apis['geometry'].apply(str_to_point)\n",
        "\n",
        "print('Before:',type(apis))\n",
        "\n",
        "#Change from pandas.DataFrame to geopandas.GeoDataFrame\n",
        "apis = gpd.GeoDataFrame(apis,\n",
        "                        geometry='geometry',\n",
        "                        crs='EPSG:26913')\n",
        "\n",
        "print('After:',type(apis))\n",
        "apis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bx2cDdshamb"
      },
      "source": [
        "-  Open [COGCC](https://cogcc.state.co.us/data5.html#/cogis_old) in a new tab.\n",
        "-  On the [website](https://cogcc.state.co.us/data5.html#/cogis_old) select WELL under facility type and select JACKSON county and search.\n",
        "-  Click on a few wells. Notice that the URL doesn't change.\n",
        "-  Now this time open a well in a new tab (Right click + 'Open link in new tab').\n",
        "-  Notice that the URL is now specific to that well.\n",
        "\n",
        "We're going to utilize this to get more information in a usable format for these wells.  Let's break out the non-unique portions of this URL to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3UP3bV7hamc"
      },
      "source": [
        "baseURL = 'https://cogcc.state.co.us/cogis/FacilityDetail.asp?facid='\n",
        "tailURL = '&type=WELL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XcfKxZphamd"
      },
      "source": [
        "Generally websites like this will have a base URL seperated by `?` followed by variables. Notice that COGCC doesn't use the state code in the API number with no deliminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OK6wAZThame"
      },
      "source": [
        "url = baseURL + '05-057-05128'.replace('-','')[2:] + tailURL\n",
        "print('URL:', url)\n",
        "r = requests.get(url)\n",
        "print('Encoding:', r.encoding)\n",
        "print('RespCode:',type(r.status_code),r.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUmQyhGGhamg"
      },
      "source": [
        "A response code of `200` lets us know that it was a good request. No let's look at the text that COGCC sent us back..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xno7lH9Yhamg"
      },
      "source": [
        "r.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Z-1UGqhamj"
      },
      "source": [
        "With the last update of the COGCC's website we can actually send the html to `pandas` directly and recieve a list of dataframes from the page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaO8VAyYsYmP"
      },
      "source": [
        "df_list = pd.read_html(r.text)\n",
        "print('Number of lists found:', len(df_list))\n",
        "df_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmM1SpEBu9M4"
      },
      "source": [
        "Now we can see that most of the page's data is avalible in the last table. We'll parse that data down to what we need and define the column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Tw-czxwVm_"
      },
      "source": [
        "# Select last df in list\n",
        "tops = df_list[0]\n",
        "\n",
        "#Find index of the top of the table\n",
        "i = tops[tops[0] == 'Formation'].index.values[0]\n",
        "\n",
        "#Set column names without spaces\n",
        "tops.columns = [x.strip().replace(' ','_') for x in tops.loc[i,:].tolist()]\n",
        "\n",
        "#Slice dataframe and reset the index\n",
        "tops = tops[i + 1:].reset_index(drop=True)\n",
        "\n",
        "#Preview our tops df\n",
        "tops.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQpCd626xkH3"
      },
      "source": [
        "Now we'll remove the unit and format the column content as float. I'll use [regular expression](https://docs.python.org/3/howto/regex.html) to do this. `\\D` looks for any non-numeric character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mXwwMGMwm0h"
      },
      "source": [
        "#For only the Log_Top & Log_Bottom columns\n",
        "for col in tops.columns[1:3]:\n",
        "\n",
        "  #Where the column is not null remove the non-numeric characters\n",
        "  tops[col] = tops[col][tops[col].notnull()].apply(lambda x: re.sub('\\D',\n",
        "                                                                    '',\n",
        "                                                                    x))\n",
        "  #df[col] = df[col][where not null].apply(lambda x: re.sub(search for, \n",
        "  #                                                         replace with,\n",
        "  #                                                         original string))\n",
        "\n",
        "  tops[col] = tops[col].astype(float)\n",
        "print(tops.dtypes)\n",
        "tops.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KrYF9ICJK-J"
      },
      "source": [
        "Now that we have the tops parsed from the website html and formated we'll roll all of that code up into a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I52JL2kJ4Km"
      },
      "source": [
        "def top_parse(text):\n",
        "  '''\n",
        "  Input:\n",
        "  text; str, html code from COGCC facility detail site\n",
        "\n",
        "  Output\n",
        "  tops; df, DataFrame of formation tops\n",
        "  '''\n",
        "  #Create list of DataFrames\n",
        "  df_list = pd.read_html(text)\n",
        "\n",
        "  #Select last DF\n",
        "  tops = df_list[0]\n",
        "  \n",
        "  #Test for no tops\n",
        "  if 'Formation' not in tops[0].tolist():\n",
        "    print('No Tops Found')\n",
        "    return pd.DataFrame()\n",
        "  \n",
        "  #Set column names\n",
        "  i = tops[tops[0] == 'Formation'].index.values[0]\n",
        "  tops.columns = [x.strip().replace(' ','_') for x in tops.loc[i,:].tolist()]\n",
        "  tops = tops[i + 1:].reset_index(drop=True)\n",
        "  #tops = tops[1:].reset_index(drop=True)\n",
        "\n",
        "  #Format Top and Bottom column\n",
        "  cols = ['Formation','Log_Top','Log_Bottom','Cored','DSTs']\n",
        "  tops = tops[cols]\n",
        "  for col in cols[1:3]:\n",
        "      tops[col] = tops[col][tops[col].notnull()].apply(lambda x: re.sub('\\D',\n",
        "                                                                    '',\n",
        "      try:                                                              x))\n",
        "        tops[col] = tops[col].astype(float)\n",
        "      except:\n",
        "        print(col,'float type conversion error.')\n",
        "  \n",
        "  tops = tops[tops.Formation != 'No formation data to display.']\n",
        "  tops = tops[(tops.Formation.notnull())&(~tops[tops.Formation.notnull()].Formation.str.contains('No additional interval'))]\n",
        "  \n",
        "  return tops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gRyX0opQKXB"
      },
      "source": [
        "print(url)\n",
        "top_parse(r.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSxNXjChamu"
      },
      "source": [
        "And iterrate through our wells. It is _EXTREMELY_ important to add `try` `except` to handle errors in scraping. Scrapers deal with others people's code and things *will* go wrong. It's also a good idea on long scrapes to periodically saveout your progress as there's nothing worse then getting back to something that ran all weeekend pulling data that you need for a project and to see that it crashed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrxo9Z70hamu"
      },
      "source": [
        "topDF = pd.DataFrame()\n",
        "i = 0\n",
        "apiSample = apis.head(10) #We'll only do the first few for this example \n",
        "total = apiSample.shape[0]\n",
        "\n",
        "for index, row in apiSample.iterrows(): \n",
        "    i += 1\n",
        "    prec = str(int(100*i/total)) + '% complete  '\n",
        "    print(row['API_Label'], prec, end='\\r')\n",
        "    try:\n",
        "        url = baseURL + row['API_Label'].replace('-','')[2:] + tailURL\n",
        "        print(url)\n",
        "        r = requests.get(url)\n",
        "\n",
        "        if r.status_code == 200:\n",
        "            formations = top_parse(r.text)\n",
        "            formations['API'] = row['API_Label']\n",
        "            # topDF = topDF.append(formations,ignore_index=True)\n",
        "            topDF = pd.concat([topDF, formations],\n",
        "                               ignore_index=True)\n",
        "            time.sleep(5) #Wait 5 sec.\n",
        "        else:\n",
        "            print(row['API_Label'],':',r.status_code)\n",
        "    except Exception as e:\n",
        "        print('Error:',row['API_Label'],e)\n",
        "\n",
        "topDF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWXDLbD5hamx"
      },
      "source": [
        "I've gone ahead and pulled all the tops for Jackson County for you.  This took approximately an hour and a half for 771 records to give you an idea of the time needed. These are avalible in the project folder.  This was a basic example with `requests` but if this is something you would like to do regularly I suggest you also check out `urllib`.  There are packages avalible to make the searching and parsing of the html much easier but when you're troubleshooting a tough website it's good to know what you are looking for\n",
        "\n",
        "---\n",
        "\n",
        "# Scraping with a Browser with Selenium\n",
        "\n",
        "Scraping with a browser allows you to navigate around obsticles that are often put in place to discourage scraping, fillout forms, and interact with a website in ways that `requests` can't.  That being said it can be significantly more challenging and can sometimes take much longer. In this example we will pull production data from COGCC. `selenium` locates \"elements\" of a web page to interact with them to preform tasks. There are several [different methods](https://selenium-python.readthedocs.io/locating-elements.html) to locate elements. We will also use `bs4` to parse a table from html. BeautifulSoup uses tag names and daughter relationships to make finding data easier.  \n",
        "\n",
        "I've previously written up this function but please open COGCC's [facility search](https://cogcc.state.co.us/cogis/FacilitySearch.asp) in a new tab. Select \"Well\", enter Weld County's code \"123\", and the sequence code \"39340\". Hit search. Select the well that comes up. Note the URL.\n",
        "\n",
        "With that open, copy the link from the well name.  Notice that there is one of these per wellbore. Paste this url into a new tab. Now let's walk through finding elements & using tags to find the data you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlRbGUQIrbh2"
      },
      "source": [
        "# To selenium Run in Colab\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Set Options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Make Web Driver\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "## If you want to run the code below in a Jupyter Notebook use this create the \n",
        "## driver\n",
        "#from selenium import webdriver\n",
        "\n",
        "## Make Driver\n",
        "#chromedriver = \"chromedriver.exe\" #Path to your chromedriver - https://sites.google.com/a/chromium.org/chromedriver/\n",
        "#driver = webdriver.Chrome(executable_path=chromedriver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QyLo0bQhamy"
      },
      "source": [
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "pd.options.display.max_columns = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCxD46BC_F_B"
      },
      "source": [
        "Elements can be found with `driver.find_elements(By.<method>,<value>)` or individually with `driver.find_element(By.<method>,<value>)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ptEoyO-BXR"
      },
      "source": [
        "print('\"By\" methods:',dir(By)[:8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "w3HE0SfBhamz"
      },
      "source": [
        "def pull_CO_prod(api_05, df, driver, pull_excel=False):\n",
        "    url = 'https://cogcc.state.co.us/cogis/FacilityDetail.asp?facid='+api_05+'&type=WELL'\n",
        "    print(url)\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    links = driver.find_elements(By.TAG_NAME,'a')\n",
        "    prod_wellbores = [x.get_attribute(\"href\") for x in driver.find_elements(By.TAG_NAME,'a') if 'production' in x.get_attribute(\"href\")]\n",
        "    print('prod_wellbores',prod_wellbores)\n",
        "    for wellbore in prod_wellbores:\n",
        "        driver.get(wellbore)\n",
        "        time.sleep(1)\n",
        "        \n",
        "        #Download the file\n",
        "        if pull_excel:\n",
        "            dwnExcel = driver.find_element(By.XPATH,'//*[@id=\"mainContent_btnExport\"]')\n",
        "            #//*[@id=\"mainContent_btnExport\"]\n",
        "            dwnExcel.click()\n",
        "            \n",
        "        #Table HTML\n",
        "        table = driver.find_elements(By.TAG_NAME,'table')[-1]\n",
        "\n",
        "        #BeautifulSoup\n",
        "        soup = BeautifulSoup(table.get_attribute('innerHTML'), \"html.parser\")\n",
        "        \n",
        "        rows = soup.find_all('tr')\n",
        "        row_list = []\n",
        "        \n",
        "        #Pull Header \n",
        "        for tr in rows[:1]:\n",
        "            th = tr.find_all('th')\n",
        "            row = [i.text for i in th]\n",
        "            row_list.append(row)\n",
        "\n",
        "        #Pull Rows\n",
        "        for tr in rows[1:]:\n",
        "            td = tr.find_all('td')\n",
        "            row = [i.text.replace('\\xa0','') for i in td]\n",
        "            row_list.append(row)\n",
        "        \n",
        "        temp = pd.DataFrame(row_list[1:],columns=row_list[0])\n",
        "        temp['First of Month'] = pd.to_datetime(temp['First of Month'])\n",
        "        temp.sort_values(by='First of Month',inplace=True)\n",
        "\n",
        "        df = pd.concat([df,temp],ignore_index=True)\n",
        "\n",
        "        return df, driver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZvqPo3mham2"
      },
      "source": [
        "# Give it a try\n",
        "\n",
        "Now that we have the function complete the `for` loop below to feed the individual apis, minus the state code, to the function. Remember that you need to pass the dataframe and the driver to the function too.\n",
        "\n",
        "Run it for the following wells: `0512339340`,`0512339383`,`0512339370`, & `0512339384`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHP8kRAVr3Kh"
      },
      "source": [
        "##I've laid out the format for you below. Make edits at *1, *2, & *3.\n",
        "\n",
        "apis =   #*1: Make a list of your UWI codes\n",
        "\n",
        "df =  #*2: Make an Empty DataFrame\n",
        "\n",
        "for api in apis:\n",
        "    \n",
        "    api_05 = api[2:]\n",
        "    print(api_05)\n",
        "    df, driver =    #*3: Insert the function w/ inputs\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcCZqVzEwTHt"
      },
      "source": [
        "Once that works for you let's format some of the strings in that dataframe to floats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMz6Sgqd0j-K"
      },
      "source": [
        "#Set data types & preview data\n",
        "cols = ['Oil Produced','Gas Produced','Water Volume','Days Produced']\n",
        "for col in cols:\n",
        "  df[col].replace('',0,inplace=True)\n",
        "  df[col] = df[col].astype(float)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sTv0ZJxOEDf"
      },
      "source": [
        "Plot cumulative oil curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl418qCGwZ82"
      },
      "source": [
        "fig=plt.figure(figsize=(15, 5))\n",
        "ax=fig.add_subplot(111)\n",
        "\n",
        "for api_wb, group in df[['First of Month','API Sequence','Days Produced','Oil Produced']].groupby('API Sequence'):\n",
        "  group['CumOil'] = group['Oil Produced'].fillna(0).cumsum()\n",
        "  \n",
        "  group['Days Produced'] = group['Days Produced'].replace('',0).astype(float)\n",
        "  \n",
        "  group['Total_Days'] = group['Days Produced'].cumsum()\n",
        "  \n",
        "  prod_start = df['First of Month'].min()\n",
        "  group['Elapsed_Days'] = group['First of Month'].apply(lambda x: (x - prod_start).days )\n",
        "  group['Elapsed_Days'] = group['Elapsed_Days'].astype(float)\n",
        "  ax.plot(group.Total_Days,\n",
        "          group.CumOil,\n",
        "          ls='-',\n",
        "          label='05-123-'+api_wb,\n",
        "          fillstyle='none')\n",
        "\n",
        "plt.legend(loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGx_biY61bVa"
      },
      "source": [
        "---\n",
        "\n",
        "# COGCCpy\n",
        "\n",
        "Want all of that data in an easier to use package? Check out [COGCCpy](https://pypi.org/project/COGCCpy/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PsbLuyc9Ce9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}